{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2f2c78a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as  np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b726db5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the image to detect, get width, height \n",
    "\n",
    "img_to_detect = cv2.imread('images/Scene1.JPG')\n",
    "img_height = img_to_detect.shape[0]\n",
    "img_width = img_to_detect.shape[1]\n",
    "\n",
    "# convert to blob to pass into model\n",
    "img_blob = cv2.dnn.blobFromImage(img_to_detect, 0.003922, (416, 416), swapRB=True, crop=False)\n",
    "#recommended by yolo authors, scale factor is 0.003922=1/255, width,height of blob is 416,416\n",
    "#accepted sizes are 320×320,416×416,609×609. More size means more accuracy but less speed\n",
    "#swapRB - openCV library read images in BGR while model accepts in RGB. So setting this flag to true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d334401e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set of 80 class labels which YOLO is trained\n",
    "class_labels = [\"person\",\"bicycle\",\"car\",\"motorcycle\",\"airplane\",\"bus\",\"train\",\"truck\",\"boat\",\n",
    "                \"trafficlight\",\"firehydrant\",\"stopsign\",\"parkingmeter\",\"bench\",\"bird\",\"cat\",\n",
    "                \"dog\",\"horse\",\"sheep\",\"cow\",\"elephant\",\"bear\",\"zebra\",\"giraffe\",\"backpack\",\n",
    "                \"umbrella\",\"handbag\",\"tie\",\"suitcase\",\"frisbee\",\"skis\",\"snowboard\",\"sportsball\",\n",
    "                \"kite\",\"baseballbat\",\"baseballglove\",\"skateboard\",\"surfboard\",\"tennisracket\",\n",
    "                \"bottle\",\"wineglass\",\"cup\",\"fork\",\"knife\",\"spoon\",\"bowl\",\"banana\",\"apple\",\n",
    "                \"sandwich\",\"orange\",\"broccoli\",\"carrot\",\"hotdog\",\"pizza\",\"donut\",\"cake\",\"chair\",\n",
    "                \"sofa\",\"pottedplant\",\"bed\",\"diningtable\",\"toilet\",\"tvmonitor\",\"laptop\",\"mouse\",\n",
    "                \"remote\",\"keyboard\",\"cellphone\",\"microwave\",\"oven\",\"toaster\",\"sink\",\"refrigerator\",\n",
    "                \"book\",\"clock\",\"vase\",\"scissors\",\"teddybear\",\"hairdrier\",\"toothbrush\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "14235afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([255,   0, 255]), array([  0,  50, 255]), array([255,  10,   0]), array([255, 255, 198]), array([200, 255, 255])]\n",
      "(5, 3)\n",
      "(80, 3)\n"
     ]
    }
   ],
   "source": [
    "#Declare List of colors as an array\n",
    "#Green, Blue, Red, cyan, yellow, purple\n",
    "#Split based on ',' and for every split, change type to int\n",
    "#convert that to a numpy array to apply color mask to the image numpy array\n",
    "class_colors = [\"255,0,255\",\"0,50,255\",\"255,10,0\",\"255,255,198\",\"200,255,255\"]\n",
    "class_colors = [np.array(every_color.split(\",\")).astype(\"int\") for every_color in class_colors]\n",
    "print(class_colors)\n",
    "class_colors = np.array(class_colors)\n",
    "print(class_colors.shape)\n",
    "class_colors = np.tile(class_colors,(16,1))\n",
    "print(class_colors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f08a08cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['conv_0', 'bn_0', 'leaky_1', 'conv_1', 'bn_1', 'leaky_2', 'conv_2', 'bn_2', 'leaky_3', 'conv_3', 'bn_3', 'leaky_4', 'shortcut_4', 'conv_5', 'bn_5', 'leaky_6', 'conv_6', 'bn_6', 'leaky_7', 'conv_7', 'bn_7', 'leaky_8', 'shortcut_8', 'conv_9', 'bn_9', 'leaky_10', 'conv_10', 'bn_10', 'leaky_11', 'shortcut_11', 'conv_12', 'bn_12', 'leaky_13', 'conv_13', 'bn_13', 'leaky_14', 'conv_14', 'bn_14', 'leaky_15', 'shortcut_15', 'conv_16', 'bn_16', 'leaky_17', 'conv_17', 'bn_17', 'leaky_18', 'shortcut_18', 'conv_19', 'bn_19', 'leaky_20', 'conv_20', 'bn_20', 'leaky_21', 'shortcut_21', 'conv_22', 'bn_22', 'leaky_23', 'conv_23', 'bn_23', 'leaky_24', 'shortcut_24', 'conv_25', 'bn_25', 'leaky_26', 'conv_26', 'bn_26', 'leaky_27', 'shortcut_27', 'conv_28', 'bn_28', 'leaky_29', 'conv_29', 'bn_29', 'leaky_30', 'shortcut_30', 'conv_31', 'bn_31', 'leaky_32', 'conv_32', 'bn_32', 'leaky_33', 'shortcut_33', 'conv_34', 'bn_34', 'leaky_35', 'conv_35', 'bn_35', 'leaky_36', 'shortcut_36', 'conv_37', 'bn_37', 'leaky_38', 'conv_38', 'bn_38', 'leaky_39', 'conv_39', 'bn_39', 'leaky_40', 'shortcut_40', 'conv_41', 'bn_41', 'leaky_42', 'conv_42', 'bn_42', 'leaky_43', 'shortcut_43', 'conv_44', 'bn_44', 'leaky_45', 'conv_45', 'bn_45', 'leaky_46', 'shortcut_46', 'conv_47', 'bn_47', 'leaky_48', 'conv_48', 'bn_48', 'leaky_49', 'shortcut_49', 'conv_50', 'bn_50', 'leaky_51', 'conv_51', 'bn_51', 'leaky_52', 'shortcut_52', 'conv_53', 'bn_53', 'leaky_54', 'conv_54', 'bn_54', 'leaky_55', 'shortcut_55', 'conv_56', 'bn_56', 'leaky_57', 'conv_57', 'bn_57', 'leaky_58', 'shortcut_58', 'conv_59', 'bn_59', 'leaky_60', 'conv_60', 'bn_60', 'leaky_61', 'shortcut_61', 'conv_62', 'bn_62', 'leaky_63', 'conv_63', 'bn_63', 'leaky_64', 'conv_64', 'bn_64', 'leaky_65', 'shortcut_65', 'conv_66', 'bn_66', 'leaky_67', 'conv_67', 'bn_67', 'leaky_68', 'shortcut_68', 'conv_69', 'bn_69', 'leaky_70', 'conv_70', 'bn_70', 'leaky_71', 'shortcut_71', 'conv_72', 'bn_72', 'leaky_73', 'conv_73', 'bn_73', 'leaky_74', 'shortcut_74', 'conv_75', 'bn_75', 'leaky_76', 'conv_76', 'bn_76', 'leaky_77', 'conv_77', 'bn_77', 'leaky_78', 'conv_78', 'bn_78', 'leaky_79', 'conv_79', 'bn_79', 'leaky_80', 'conv_80', 'bn_80', 'leaky_81', 'conv_81', 'permute_82', 'yolo_82', 'identity_83', 'conv_84', 'bn_84', 'leaky_85', 'upsample_85', 'concat_86', 'conv_87', 'bn_87', 'leaky_88', 'conv_88', 'bn_88', 'leaky_89', 'conv_89', 'bn_89', 'leaky_90', 'conv_90', 'bn_90', 'leaky_91', 'conv_91', 'bn_91', 'leaky_92', 'conv_92', 'bn_92', 'leaky_93', 'conv_93', 'permute_94', 'yolo_94', 'identity_95', 'conv_96', 'bn_96', 'leaky_97', 'upsample_97', 'concat_98', 'conv_99', 'bn_99', 'leaky_100', 'conv_100', 'bn_100', 'leaky_101', 'conv_101', 'bn_101', 'leaky_102', 'conv_102', 'bn_102', 'leaky_103', 'conv_103', 'bn_103', 'leaky_104', 'conv_104', 'bn_104', 'leaky_105', 'conv_105', 'permute_106', 'yolo_106']\n",
      "---------------------------\n",
      "[[200]\n",
      " [227]\n",
      " [254]]\n"
     ]
    }
   ],
   "source": [
    "# Loading pretrained model \n",
    "# input preprocessed blob into model and pass through the model\n",
    "# obtain the detection predictions by the model using forward() method\n",
    "yolo_model = cv2.dnn.readNetFromDarknet('cfg/yolov3.cfg','weight/yolov3.weights')\n",
    "yolo_layers = yolo_model.getLayerNames()\n",
    "print(yolo_layers)\n",
    "print(\"---------------------------\")\n",
    "yolo_output_layer = [yolo_layers[yolo_layer[0] - 1] for yolo_layer in yolo_model.getUnconnectedOutLayers()]\n",
    "print(yolo_model.getUnconnectedOutLayers())\n",
    "\n",
    "# input preprocessed blob into model and pass through the model\n",
    "yolo_model.setInput(img_blob)\n",
    "# obtain the detection layers by forwarding through till the output layer\n",
    "obj_detection_layers = yolo_model.forward(yolo_output_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "63495bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization for non-max suppression (NMS)\n",
    "# declare list for [class id], [box center, width & height[], [confidences]\n",
    "class_ids_list = []\n",
    "boxes_list = []\n",
    "confidences_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5c0ee5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted object 0 - person: 98.87%\n",
      "predicted object 0 - person: 38.21%\n",
      "predicted object 0 - person: 56.07%\n",
      "predicted object 0 - person: 99.43%\n",
      "predicted object 0 - person: 26.21%\n",
      "predicted object 0 - person: 34.46%\n",
      "predicted object 24 - backpack: 26.24%\n",
      "predicted object 24 - backpack: 64.42%\n",
      "predicted object 24 - backpack: 71.19%\n",
      "predicted object 24 - backpack: 35.32%\n",
      "predicted object 24 - backpack: 71.81%\n",
      "predicted object 24 - backpack: 66.94%\n",
      "predicted object 0 - person: 88.60%\n",
      "predicted object 0 - person: 95.67%\n",
      "predicted object 0 - person: 96.34%\n",
      "predicted object 0 - person: 98.74%\n",
      "predicted object 0 - person: 23.14%\n",
      "predicted object 0 - person: 40.12%\n",
      "predicted object 0 - person: 96.06%\n",
      "predicted object 0 - person: 21.70%\n",
      "predicted object 0 - person: 98.47%\n",
      "predicted object 0 - person: 21.79%\n",
      "predicted object 0 - person: 87.06%\n",
      "predicted object 0 - person: 33.14%\n",
      "predicted object 0 - person: 93.78%\n",
      "predicted object 0 - person: 98.24%\n",
      "predicted object 0 - person: 32.93%\n",
      "predicted object 0 - person: 99.51%\n",
      "predicted object 0 - person: 83.09%\n",
      "predicted object 0 - person: 97.02%\n",
      "predicted object 0 - person: 31.44%\n",
      "predicted object 0 - person: 96.93%\n",
      "predicted object 0 - person: 60.08%\n",
      "predicted object 0 - person: 99.35%\n",
      "predicted object 0 - person: 90.23%\n",
      "predicted object 0 - person: 66.08%\n",
      "predicted object 0 - person: 67.89%\n",
      "predicted object 0 - person: 25.78%\n",
      "predicted object 0 - person: 99.63%\n",
      "predicted object 0 - person: 43.43%\n",
      "predicted object 0 - person: 90.92%\n",
      "predicted object 0 - person: 39.06%\n",
      "predicted object 0 - person: 93.61%\n",
      "predicted object 24 - backpack: 67.66%\n",
      "predicted object 0 - person: 99.36%\n",
      "predicted object 0 - person: 86.09%\n",
      "predicted object 0 - person: 58.59%\n",
      "predicted object 0 - person: 92.40%\n",
      "predicted object 0 - person: 54.23%\n",
      "predicted object 24 - backpack: 73.07%\n",
      "predicted object 26 - handbag: 23.48%\n",
      "predicted object 0 - person: 45.11%\n",
      "predicted object 0 - person: 24.53%\n",
      "predicted object 0 - person: 23.26%\n",
      "predicted object 0 - person: 37.75%\n",
      "predicted object 0 - person: 73.87%\n",
      "predicted object 0 - person: 80.15%\n",
      "predicted object 0 - person: 40.23%\n",
      "predicted object 0 - person: 42.64%\n",
      "predicted object 0 - person: 42.72%\n",
      "predicted object 0 - person: 66.28%\n",
      "predicted object 24 - backpack: 44.09%\n",
      "predicted object 24 - backpack: 20.90%\n",
      "predicted object 0 - person: 50.22%\n",
      "predicted object 0 - person: 22.54%\n",
      "NMS predicted object person1: 99.63%\n",
      "NMS predicted object person2: 99.51%\n",
      "NMS predicted object person3: 99.43%\n",
      "NMS predicted object person4: 99.36%\n",
      "NMS predicted object person5: 99.35%\n",
      "NMS predicted object person6: 98.87%\n",
      "NMS predicted object person7: 98.74%\n",
      "NMS predicted object person8: 98.47%\n",
      "NMS predicted object person9: 98.24%\n",
      "NMS predicted object person10: 97.02%\n",
      "NMS predicted object person11: 95.67%\n",
      "NMS predicted object person12: 92.40%\n",
      "NMS predicted object person13: 87.06%\n",
      "NMS predicted object person14: 80.15%\n",
      "NMS predicted object backpack15: 73.07%\n",
      "NMS predicted object backpack16: 71.81%\n",
      "NMS predicted object backpack17: 67.66%\n",
      "NMS predicted object person18: 66.28%\n",
      "NMS predicted object person19: 58.59%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loop over each of the layer outputs\n",
    "for object_detection_layer in obj_detection_layers:\n",
    "    # loop over the detections\n",
    "    for object_detection in object_detection_layer:\n",
    "        \n",
    "        # obj_detections[1 to 4] => will have the two center points, box width and box height\n",
    "        # obj_detections[5] => will have scores for all objects within bounding box\n",
    "        all_scores = object_detection[5:]\n",
    "        predicted_class_id = np.argmax(all_scores)\n",
    "        prediction_confidence = all_scores[predicted_class_id]\n",
    "    \n",
    "        # take only predictions with confidence more than 20%\n",
    "        if prediction_confidence > 0.20:\n",
    "            #get the predicted label\n",
    "            predicted_class_label = class_labels[predicted_class_id]\n",
    "            #obtain the bounding box co-oridnates for actual image from resized image size\n",
    "            bounding_box = object_detection[0:4] * np.array([img_width, img_height, img_width, img_height])\n",
    "            (box_center_x_pt, box_center_y_pt, box_width, box_height) = bounding_box.astype(\"int\")\n",
    "            start_x_pt = int(box_center_x_pt - (box_width / 2))\n",
    "            start_y_pt = int(box_center_y_pt - (box_height / 2))\n",
    "            \n",
    "            ############## NMS Change 2 ###############\n",
    "            # print the prediction in console\n",
    "            predicted_class_label = \"{} - {}: {:.2f}%\".format(predicted_class_id, predicted_class_label, prediction_confidence * 100)\n",
    "            print(\"predicted object {}\".format(predicted_class_label))\n",
    "            #save class id, start x, y, width & height, confidences in a list for nms processing\n",
    "            #make sure to pass confidence as float and width and height as integers\n",
    "            class_ids_list.append(predicted_class_id)\n",
    "            confidences_list.append(float(prediction_confidence))\n",
    "            boxes_list.append([start_x_pt, start_y_pt, int(box_width), int(box_height)])\n",
    "            \n",
    "# Applying the NMS will return only the selected max value ids while suppressing the non maximum\n",
    "# (weak) overlapping bounding boxes      \n",
    "# Non-Maxima Suppression confidence set as 0.5 & max_suppression threhold for NMS as 0.4 \n",
    "max_value_ids = cv2.dnn.NMSBoxes(boxes_list, confidences_list, 0.5, 0.4)\n",
    "\n",
    "# loop through the final set of detections remaining after NMS and draw bounding box and write text\n",
    "i=1\n",
    "for max_valueid in max_value_ids:\n",
    "    max_class_id = max_valueid[0]\n",
    "    box = boxes_list[max_class_id]\n",
    "    start_x_pt = box[0]\n",
    "    start_y_pt = box[1]\n",
    "    box_width = box[2]\n",
    "    box_height = box[3]\n",
    "    end_x_pt = start_x_pt + box_width\n",
    "    end_y_pt = start_y_pt + box_height\n",
    "    #get the predicted class id and label\n",
    "    predicted_class_id = class_ids_list[max_class_id]\n",
    "    predicted_class_label = class_labels[predicted_class_id]\n",
    "    prediction_confidence = confidences_list[max_class_id]\n",
    "\n",
    "    #get a random mask color from the numpy array of colors\n",
    "    box_color = class_colors[predicted_class_id]\n",
    "\n",
    "    #convert the color numpy array as a list and apply to text and box\n",
    "    box_color = [int(c) for c in box_color]\n",
    "\n",
    "    # print the prediction in console\n",
    "    predicted_class_label = \"{}: {:.2f}%\".format(predicted_class_label+str(i), prediction_confidence * 100)\n",
    "    print(\"NMS predicted object {}\".format(predicted_class_label))\n",
    "\n",
    "    # draw rectangle and text in the image\n",
    "    cv2.rectangle(img_to_detect, (start_x_pt, start_y_pt), (end_x_pt, end_y_pt), box_color, 2)\n",
    "    cv2.putText(img_to_detect, predicted_class_label, (start_x_pt, start_y_pt-5), cv2.FONT_HERSHEY_SIMPLEX, .5, box_color, 1)\n",
    "    #draw centre of each box\n",
    "    centrex=int(start_x_pt + box_width/2)\n",
    "    centrey=int(start_y_pt + box_height/2)\n",
    "    centre=(centrex, centrey)\n",
    "    color_centre=(0,0,0)\n",
    "    cv2.circle(img_to_detect, centre, 8, color_centre, 2)\n",
    "    cv2.putText(img_to_detect, str(i), (centrex, centrey-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color_centre, 1)\n",
    "    i+=1\n",
    "\n",
    "#cv2.imshow(\"Detection Output\", img_to_detect)\n",
    "cv2.imwrite('Scene1.jpg',img_to_detect)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
